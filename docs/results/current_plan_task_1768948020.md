# План выполнения задачи: Оптимизация самых частых операций

## Контекст задачи

Задача: "Оптимизировать самые частые операции (копирование словарей параметров, сериализация snapshot, работа с memory)"

## Анализ текущего состояния

### Выявленные bottleneck'ы

1. **Копирование словарей параметров**
   - Функция `_safe_copy_dict()` в runtime loop использует `copy.deepcopy()` для вложенных словарей
   - Частое копирование `learning_params` и `adaptation_params` в adaptation
   - Вызывается при каждом тике adaptation (каждые 100 тиков)

2. **Сериализация snapshots**
   - `save_snapshot()` делает полную сериализацию состояния каждые 10 тиков
   - Конвертация Memory из объектов в dict (лишняя работа)
   - Исключение transient полей происходит после полной сериализации

3. **Операции с памятью**
   - `decay_weights()` обрабатывает все записи памяти каждые 10 тиков
   - `archive_old_entries()` сканирует все записи каждые 50 тиков
   - `clamp_size()` вызывается при каждом добавлении записи

### Текущие метрики производительности (baseline)

- Средняя длительность тика: 14.9 мс (медиана 9.76 мс)
- 99.5% времени тратится на `time.sleep` между тиками
- Основная нагрузка на CPU: операции с памятью и сериализация

## План оптимизации

### Этап 1: Оптимизация копирования словарей (высокий приоритет)
**Цель:** Снизить накладные расходы на копирование параметров

1. **Замена `_safe_copy_dict()` на оптимизированную версию**
   - Использовать поверхностное копирование для простых словарей
   - Избегать `copy.deepcopy()` для вложенных структур
   - Добавить кэширование неизменяемых словарей

2. **Оптимизация копирования параметров adaptation/learning**
   - Использовать copy-on-write паттерн
   - Копировать только изменяемые части
   - Добавить грязные флаги для избежания ненужного копирования

### Этап 2: Оптимизация сериализации snapshots (высокий приоритет)
**Цель:** Ускорить сохранение состояния и уменьшить размер файлов

1. **Предварительная фильтрация полей**
   - Исключать transient поля до сериализации
   - Использовать черный список полей вместо белого

2. **Оптимизация конвертации Memory**
   - Предварительно кэшировать сериализованные записи
   - Использовать инкрементальную сериализацию
   - Убрать избыточные проверки типов

3. **Улучшение I/O операций**
   - Использовать буферизованную запись
   - Оптимизировать JSON separators

### Этап 3: Оптимизация операций с памятью (средний приоритет)
**Цель:** Ускорить обработку записей памяти

1. **Оптимизация `decay_weights()`**
   - Использовать векторизованные операции (numpy если возможно)
   - Добавить индексы для быстрого доступа
   - Обработку батчами вместо по одной записи

2. **Оптимизация `archive_old_entries()`**
   - Предварительная фильтрация записей
   - Групповые операции вместо индивидуальных
   - Кэширование результатов фильтрации

3. **Оптимизация `clamp_size()`**
   - Использовать heap для быстрого поиска минимума
   - Отложенную обработку (батчинг)

### Этап 4: Добавление мониторинга производительности
**Цель:** Измерять эффект оптимизаций

1. **Метрики для критических операций**
   - Время выполнения `save_snapshot()`
   - Время выполнения `decay_weights()`
   - Время копирования словарей

2. **Интеграция с StructuredLogger**
   - Автоматическое логирование метрик производительности
   - Тренды по времени выполнения операций

### Этап 5: Тестирование оптимизаций
**Цель:** Убедиться в корректности и эффективности

1. **Производительность**
   - Бенчмарки критических операций
   - Сравнение с baseline метриками
   - Нагрузочное тестирование

2. **Корректность**
   - Регрессионные тесты
   - Тесты на эквивалентность результатов
   - Проверка консистентности состояния

## Ожидаемые результаты

- **Производительность:** Снижение CPU overhead на 20-50% для операций с памятью
- **Размер snapshots:** Уменьшение на 30-50% за счет оптимизации сериализации
- **Время копирования:** Ускорение операций копирования словарей в 2-5 раз
- **Наблюдаемость:** Детальные метрики производительности для мониторинга

## Риски и mitigation

- **Регрессии:** Тщательное тестирование всех изменений
- **Сложность кода:** Документация оптимизаций, комментарии в коде
- **Производительность:** Бенчмаркинг перед и после изменений
- **Совместимость:** Сохранение API и поведения системы

## Контрольные точки

### Неделя 1: Фундаментальные оптимизации
- ✅ Оптимизация копирования словарей
- ✅ Оптимизация сериализации snapshots
- ✅ Базовые метрики производительности

### Неделя 2: Оптимизации памяти
- ✅ Улучшения операций с памятью
- ✅ Интеграция мониторинга
- ✅ Начальные тесты производительности

### Неделя 3: Финализация и тестирование
- ✅ Полное тестирование оптимизаций
- ✅ Финальные метрики производительности
- ✅ Документация изменений

## Следующие шаги

1. Начать с оптимизации `_safe_copy_dict()` - самый частый bottleneck
2. Добавить базовые метрики производительности
3. Постепенно оптимизировать сериализацию и операции с памятью
4. Тщательно протестировать каждое изменение
