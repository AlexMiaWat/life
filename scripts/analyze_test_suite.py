#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã –ø—Ä–æ–µ–∫—Ç–∞ Life.

–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞.
"""

import os
import re
import ast
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class TestFileAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞."""
    path: str
    test_count: int
    lines_count: int
    complexity: float
    imports: List[str]
    fixtures: List[str]
    marks: List[str]
    test_types: List[str]  # unit, integration, system, smoke


@dataclass
class TestSuiteAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–π —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã."""
    total_files: int
    total_tests: int
    total_lines: int
    files_by_type: Dict[str, List[TestFileAnalysis]]
    coverage_estimate: float
    recommendations: List[str]


class TestAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã."""

    def __init__(self, test_dir: str = "src/test"):
        self.test_dir = Path(test_dir)
        self.analysis = TestSuiteAnalysis(
            total_files=0,
            total_tests=0,
            total_lines=0,
            files_by_type=defaultdict(list),
            coverage_estimate=0.0,
            recommendations=[]
        )

    def analyze(self) -> TestSuiteAnalysis:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã."""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã...")

        if not self.test_dir.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.test_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return self.analysis

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
        for test_file in self.test_dir.rglob("test_*.py"):
            file_analysis = self._analyze_test_file(test_file)
            if file_analysis:
                self.analysis.total_files += 1
                self.analysis.total_tests += file_analysis.test_count
                self.analysis.total_lines += file_analysis.lines_count

                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º
                test_type = self._classify_test_type(file_analysis)
                self.analysis.files_by_type[test_type].append(file_analysis)

        # –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ–∫
        self._calculate_coverage_estimate()
        self._generate_recommendations()

        return self.analysis

    def _analyze_test_file(self, file_path: Path) -> TestFileAnalysis | None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content)

            # –ü–æ–¥—Å—á–µ—Ç —Ç–µ—Å—Ç–æ–≤
            test_functions = []
            fixtures = []
            marks = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if node.name.startswith('test_'):
                        test_functions.append(node.name)
                    elif 'fixture' in node.name or 'Fixture' in str(node.decorator_list):
                        fixtures.append(node.name)

                # –ü–æ–∏—Å–∫ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–≤ pytest
                if isinstance(node, ast.decorator_list) and node:
                    for decorator in node:
                        if isinstance(decorator, ast.Call):
                            if hasattr(decorator.func, 'id') and decorator.func.id == 'pytest':
                                marks.extend(self._extract_marks(decorator))

            # –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    imports.extend(alias.name for alias in node.names)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    imports.extend(f"{module}.{alias.name}" for alias in node.names)

            # –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞)
            complexity = len(test_functions) * 0.1 + len(content.split('\n')) * 0.01

            return TestFileAnalysis(
                path=str(file_path.relative_to(self.test_dir.parent)),
                test_count=len(test_functions),
                lines_count=len(content.split('\n')),
                complexity=complexity,
                imports=imports,
                fixtures=fixtures,
                marks=marks,
                test_types=[]
            )

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {file_path}: {e}")
            return None

    def _classify_test_type(self, analysis: TestFileAnalysis) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç –ø–æ —Ç–∏–ø—É."""
        filename = analysis.path.lower()

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞
        if 'smoke' in filename:
            return 'smoke'
        elif 'integration' in filename or 'api' in filename:
            return 'integration'
        elif 'system' in filename or 'e2e' in filename:
            return 'system'
        elif any(mark in analysis.marks for mark in ['unit', 'isolated']):
            return 'unit'

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if any('integration' in mark for mark in analysis.marks):
            return 'integration'
        elif len(analysis.imports) > 10:  # –ú–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–æ–≤ = –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç
            return 'integration'
        elif len(analysis.fixtures) > 5:  # –ú–Ω–æ–≥–æ fixtures = –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç
            return 'integration'

        return 'unit'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é unit

    def _extract_marks(self, decorator) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å marks –∏–∑ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞ pytest."""
        marks = []
        try:
            if hasattr(decorator, 'keywords'):
                for keyword in decorator.keywords:
                    if keyword.arg:
                        marks.append(keyword.arg)
        except:
            pass
        return marks

    def _calculate_coverage_estimate(self):
        """–û—Ü–µ–Ω–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–æ–¥–∞ —Ç–µ—Å—Ç–∞–º–∏."""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–µ—Å—Ç = ~10 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
        estimated_covered_lines = self.analysis.total_tests * 10

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 100k —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ –ø—Ä–æ–µ–∫—Ç–µ
        total_code_lines = 100000
        self.analysis.coverage_estimate = min(100.0, (estimated_covered_lines / total_code_lines) * 100)

    def _generate_recommendations(self):
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        recommendations = []

        # –ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤
        if self.analysis.total_files > 100:
            recommendations.append("üö® –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è.")

        # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ —Ç–∏–ø–æ–≤ —Ç–µ—Å—Ç–æ–≤
        unit_count = len(self.analysis.files_by_type.get('unit', []))
        integration_count = len(self.analysis.files_by_type.get('integration', []))
        system_count = len(self.analysis.files_by_type.get('system', []))

        if unit_count < integration_count:
            recommendations.append("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ unit —Ç–µ—Å—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ unit —Ç–µ—Å—Ç–∞–º–∏.")

        if system_count > unit_count * 0.5:
            recommendations.append("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ system —Ç–µ—Å—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.")

        # –ê–Ω–∞–ª–∏–∑ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        avg_tests_per_file = self.analysis.total_tests / max(1, self.analysis.total_files)
        if avg_tests_per_file > 20:
            recommendations.append("üìù –§–∞–π–ª—ã —Ç–µ—Å—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.")

        self.analysis.recommendations = recommendations


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    analyzer = TestAnalyzer()
    analysis = analyzer.analyze()

    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –¢–ï–°–¢–û–í–û–ô –ë–ê–ó–´")
    print("=" * 50)
    print(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis.total_files}")
    print(f"üß™ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {analysis.total_tests}")
    print(f"üìù –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {analysis.total_lines}")
    print(f"üéØ –û—Ü–µ–Ω–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è: {analysis.coverage_estimate:.1f}%")

    print("\nüìÇ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
    for test_type, files in analysis.files_by_type.items():
        count = len(files)
        percentage = (count / analysis.total_files) * 100 if analysis.total_files > 0 else 0
        print(f"  {test_type}: {count} —Ñ–∞–π–ª–æ–≤ ({percentage:.1f}%)")
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    if analysis.recommendations:
        for rec in analysis.recommendations:
            print(f"  ‚Ä¢ {rec}")
    else:
        print("  ‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –±–∞–∑–∞ –≤—ã–≥–ª—è–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    report_file = "docs/test_analysis_report.json"
    report_data = {
        'summary': {
            'total_files': analysis.total_files,
            'total_tests': analysis.total_tests,
            'total_lines': analysis.total_lines,
            'coverage_estimate': analysis.coverage_estimate,
            'files_by_type': {k: len(v) for k, v in analysis.files_by_type.items()}
        },
        'recommendations': analysis.recommendations,
        'files': [
            {
                'path': f.path,
                'test_count': f.test_count,
                'lines_count': f.lines_count,
                'complexity': f.complexity,
                'type': next((t for t, files in analysis.files_by_type.items() if f in files), 'unknown')
            }
            for files_list in analysis.files_by_type.values()
            for f in files_list
        ]
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)

    print(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_file}")


if __name__ == "__main__":
    main()