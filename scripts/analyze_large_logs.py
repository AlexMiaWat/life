#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö JSONL —Ñ–∞–π–ª–æ–≤
—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/analyze_large_logs.py stats --log-file data/large_log.jsonl
"""

import argparse
import json
import sys
import time
import pickle
import hashlib
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Any, Iterator, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.observability.log_analysis import _empty_analysis_result


class LargeLogAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏."""

    def __init__(self, log_file: str, cache_dir: str = ".log_cache"):
        self.log_file = Path(log_file)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def get_file_hash(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ö—ç—à —Ñ–∞–π–ª–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
        hash_md5 = hashlib.md5()
        with open(self.log_file, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_cache_path(self, analysis_type: str) -> Path:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à-—Ñ–∞–π–ª—É."""
        file_hash = self.get_file_hash()
        return self.cache_dir / f"{analysis_type}_{file_hash}.pkl"

    def is_cache_valid(self, analysis_type: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∫—ç—à–∞."""
        cache_path = self.get_cache_path(analysis_type)
        if not cache_path.exists():
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        cache_mtime = cache_path.stat().st_mtime
        log_mtime = self.log_file.stat().st_mtime

        return cache_mtime > log_mtime

    def save_to_cache(self, analysis_type: str, data: Any):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à."""
        cache_path = self.get_cache_path(analysis_type)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

    def load_from_cache(self, analysis_type: str) -> Optional[Any]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞."""
        cache_path = self.get_cache_path(analysis_type)
        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à: {e}")
            return None

    def stream_entries(self) -> Iterator[Dict[str, Any]]:
        """–ü–æ—Ç–æ–∫–æ–≤–æ–µ —á—Ç–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞."""
        try:
            with open(self.log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        yield json.loads(line)
                    except json.JSONDecodeError as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                        continue

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            return

    def analyze_chunk(self, entries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–∞ –∑–∞–ø–∏—Å–µ–π."""
        chunk_stats = {
            'total_entries': len(entries),
            'stages': Counter(),
            'event_types': Counter(),
            'errors': Counter(),
            'correlation_ids': set(),
            'tick_numbers': [],
        }

        for entry in entries:
            # –°—Ç–∞–¥–∏–∏
            stage = entry.get('stage', 'unknown')
            chunk_stats['stages'][stage] += 1

            # –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π
            if stage == 'event':
                event_type = entry.get('event_type', 'unknown')
                chunk_stats['event_types'][event_type] += 1

            # –û—à–∏–±–∫–∏
            if stage.startswith('error_'):
                error_type = entry.get('error_type', 'unknown')
                chunk_stats['errors'][error_type] += 1

            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ ID
            if 'correlation_id' in entry:
                chunk_stats['correlation_ids'].add(entry['correlation_id'])

            # –ù–æ–º–µ—Ä–∞ —Ç–∏–∫–æ–≤
            if 'tick_number' in entry:
                chunk_stats['tick_numbers'].append(entry['tick_number'])

        return chunk_stats

    def merge_chunk_stats(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–°–ª–∏—è–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ —á–∞–Ω–∫–æ–≤."""
        merged = {
            'total_entries': 0,
            'stages': Counter(),
            'event_types': Counter(),
            'errors': Counter(),
            'correlation_ids': set(),
            'tick_numbers': [],
        }

        for chunk in chunks:
            merged['total_entries'] += chunk['total_entries']
            merged['stages'].update(chunk['stages'])
            merged['event_types'].update(chunk['event_types'])
            merged['errors'].update(chunk['errors'])
            merged['correlation_ids'].update(chunk['correlation_ids'])
            merged['tick_numbers'].extend(chunk['tick_numbers'])

        return merged

    def analyze_parallel(self, chunk_size: int = 10000, max_workers: Optional[int] = None) -> Dict[str, Any]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞."""
        if max_workers is None:
            max_workers = min(multiprocessing.cpu_count(), 4)  # –ù–µ –±–æ–ª—å—à–µ 4 –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å {max_workers} –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if self.is_cache_valid('parallel_stats'):
            print("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            cached = self.load_from_cache('parallel_stats')
            if cached:
                return cached

        start_time = time.time()

        # –†–∞–∑–¥–µ–ª—è–µ–º —Ñ–∞–π–ª –Ω–∞ —á–∞–Ω–∫–∏
        chunks = []
        current_chunk = []

        for entry in self.stream_entries():
            current_chunk.append(entry)
            if len(current_chunk) >= chunk_size:
                chunks.append(current_chunk)
                current_chunk = []

        if current_chunk:
            chunks.append(current_chunk)

        print(f"üì¶ –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ ~{chunk_size} –∑–∞–ø–∏—Å–µ–π")

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        chunk_results = []
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(self.analyze_chunk, chunk) for chunk in chunks]

            for future in as_completed(futures):
                try:
                    result = future.result()
                    chunk_results.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞: {e}")

        # –°–ª–∏—è–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        merged_stats = self.merge_chunk_stats(chunk_results)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        total_time = time.time() - start_time
        final_result = {
            'total_entries': merged_stats['total_entries'],
            'stages': dict(merged_stats['stages']),
            'event_types': dict(merged_stats['event_types']),
            'errors': dict(merged_stats['errors']),
            'total_correlations': len(merged_stats['correlation_ids']),
            'analysis_time': total_time,
            'processing_rate': merged_stats['total_entries'] / total_time if total_time > 0 else 0,
            'chunks_processed': len(chunk_results),
            'parallel_workers': max_workers,
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.save_to_cache('parallel_stats', final_result)

        return final_result

    def analyze_correlations_parallel(self, chunk_size: int = 50000) -> Dict[str, Any]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–µ–ø–æ—á–µ–∫ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π."""
        print("üîó –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ø–æ—á–∫–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")

        if self.is_cache_valid('correlations'):
            print("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ü–µ–ø–æ—á–µ–∫")
            cached = self.load_from_cache('correlations')
            if cached:
                return cached

        start_time = time.time()

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ü–µ–ø–æ—á–∫–∏
        correlations = defaultdict(list)

        for entry in self.stream_entries():
            if 'correlation_id' in entry:
                correlations[entry['correlation_id']].append(entry)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ø–æ—á–∫–∏
        chain_stats = []
        for chain_id, entries in correlations.items():
            if len(entries) < 2:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–µ–ø–æ—á–∫–∏
                continue

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            sorted_entries = sorted(entries, key=lambda x: x['timestamp'])

            # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ø–æ—á–∫–∏
            stages = [e['stage'] for e in sorted_entries]
            duration = sorted_entries[-1]['timestamp'] - sorted_entries[0]['timestamp']
            completeness = len(set(stages) & {'event', 'meaning', 'decision', 'action', 'feedback'}) / 5

            chain_stats.append({
                'chain_id': chain_id,
                'duration': duration,
                'completeness': completeness,
                'stages': stages,
                'entry_count': len(sorted_entries)
            })

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ø–æ—á–µ–∫
        if chain_stats:
            durations = [s['duration'] for s in chain_stats]
            completeness_values = [s['completeness'] for s in chain_stats]

            result = {
                'total_chains': len(chain_stats),
                'avg_duration': sum(durations) / len(durations),
                'median_duration': sorted(durations)[len(durations) // 2],
                'min_duration': min(durations),
                'max_duration': max(durations),
                'avg_completeness': sum(completeness_values) / len(completeness_values),
                'complete_chains': sum(1 for s in chain_stats if s['completeness'] >= 0.8),
                'analysis_time': time.time() - start_time
            }
        else:
            result = {
                'total_chains': 0,
                'avg_duration': 0,
                'median_duration': 0,
                'min_duration': 0,
                'max_duration': 0,
                'avg_completeness': 0,
                'complete_chains': 0,
                'analysis_time': time.time() - start_time
            }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.save_to_cache('correlations', result)

        return result

    def stream_filter(self, stage_filter: Optional[str] = None, event_type_filter: Optional[str] = None) -> Iterator[Dict[str, Any]]:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π."""
        for entry in self.stream_entries():
            if stage_filter and entry.get('stage') != stage_filter:
                continue
            if event_type_filter and entry.get('event_type') != event_type_filter:
                continue
            yield entry

    def export_filtered(self, output_file: str, **filters):
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        print(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ {output_file}...")

        count = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            for entry in self.stream_filter(**filters):
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                count += 1
                if count % 10000 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π...")

        print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π")


def main():
    parser = argparse.ArgumentParser(
        description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤ Life",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
  python scripts/analyze_large_logs.py stats --log-file data/large_log.jsonl

  # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ø–æ—á–µ–∫ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
  python scripts/analyze_large_logs.py chains --log-file data/large_log.jsonl

  # –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  python scripts/analyze_large_logs.py export --filter-stage event --output events.jsonl

  # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
  python scripts/analyze_large_logs.py clear-cache

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
  - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
  - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
  - –ü–æ—Ç–æ–∫–æ–≤–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ (–Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ –ø–∞–º—è—Ç—å)
  - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞–º–∏
        """
    )

    parser.add_argument(
        'command',
        choices=['stats', 'chains', 'export', 'clear-cache'],
        help='–ö–æ–º–∞–Ω–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è'
    )

    parser.add_argument(
        '--log-file',
        required=True,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
    )

    parser.add_argument(
        '--chunk-size',
        type=int,
        default=10000,
        help='–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (default: 10000)'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (default: auto)'
    )

    parser.add_argument(
        '--filter-stage',
        help='–§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞–¥–∏–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞'
    )

    parser.add_argument(
        '--filter-event-type',
        help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —Å–æ–±—ã—Ç–∏—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞'
    )

    parser.add_argument(
        '--output',
        help='–§–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'
    )

    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ'
    )

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    log_path = Path(args.log_file)
    if not log_path.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.log_file}", file=sys.stderr)
        sys.exit(1)

    analyzer = LargeLogAnalyzer(args.log_file)

    try:
        if args.command == 'stats':
            print("üìä –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
            result = analyzer.analyze_parallel(args.chunk_size, args.max_workers)

            print(f"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
            print(f"{'='*50}")
            print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {result['total_entries']:,}")
            print(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {result['analysis_time']:.2f} —Å–µ–∫")
            print(f"–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_rate']:.0f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
            print(f"–ß–∞–Ω–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['chunks_processed']}")
            print(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {result['parallel_workers']}")
            print()

            if result['stages']:
                print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç–∞–¥–∏—è–º:")
                for stage, count in sorted(result['stages'].items(), key=lambda x: x[1], reverse=True)[:10]:
                    pct = (count / result['total_entries']) * 100
                    print("15")
            print()

            if result['event_types']:
                print("–¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π:")
                for event_type, count in sorted(result['event_types'].items(), key=lambda x: x[1], reverse=True)[:10]:
                    total = sum(result['event_types'].values())
                    pct = (count / total) * 100
                    print("15")
            print()

            if result['errors']:
                print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫:")
                for error_type, count in sorted(result['errors'].items(), key=lambda x: x[1], reverse=True)[:5]:
                    total = sum(result['errors'].values())
                    pct = (count / total) * 100
                    print("15")

        elif args.command == 'chains':
            print("üîó –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ü–µ–ø–æ—á–µ–∫ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")
            result = analyzer.analyze_correlations_parallel(args.chunk_size)

            print(f"üîó –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –¶–ï–ü–û–ß–ï–ö")
            print(f"{'='*50}")
            print(f"–í—Å–µ–≥–æ —Ü–µ–ø–æ—á–µ–∫: {result['total_chains']:,}")
            print(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {result['analysis_time']:.2f} —Å–µ–∫")
            print()

            if result['total_chains'] > 0:
                print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
                print(".3f")
                print(".3f")
                print(".3f")
                print(".3f")
                print()

                print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã:")
                print(".1%")
                print(f"–ü–æ–ª–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫: {result['complete_chains']}")

        elif args.command == 'export':
            if not args.output:
                print("‚ùå –û—à–∏–±–∫–∞: —É–∫–∞–∂–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å --output", file=sys.stderr)
                sys.exit(1)

            filters = {}
            if args.filter_stage:
                filters['stage_filter'] = args.filter_stage
            if args.filter_event_type:
                filters['event_type_filter'] = args.filter_event_type

            analyzer.export_filtered(args.output, **filters)

        elif args.command == 'clear-cache':
            import shutil
            if analyzer.cache_dir.exists():
                shutil.rmtree(analyzer.cache_dir)
                analyzer.cache_dir.mkdir()
                print("üóëÔ∏è  –ö—ç—à –æ—á–∏—â–µ–Ω")
            else:
                print("üìÅ –ö—ç—à —É–∂–µ –ø—É—Å—Ç")

    except KeyboardInterrupt:
        print("\nüõë –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()